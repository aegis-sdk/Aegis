/**
 * Perplexity estimation module.
 *
 * Provides a lightweight, dependency-free character-level n-gram perplexity
 * estimator for detecting adversarial inputs that are statistically unusual:
 * GCG adversarial suffixes, encoded payloads, random gibberish, etc.
 *
 * Key insight: adversarial suffixes generated by gradient-based attacks (Zou
 * et al. 2023) produce character sequences with unnaturally high entropy
 * (>5.0 bits/char) compared to natural English (~3.5-4.0 bits/char). This
 * module measures character-level perplexity using n-gram frequency analysis
 * and Shannon entropy across sliding windows, flagging windows that exceed
 * configurable thresholds.
 *
 * No external ML models or bundled weights — pure JavaScript arithmetic.
 * Runs in O(n) time where n is the input length.
 */

import type {
  PerplexityConfig,
  PerplexityResult,
  PerplexityWindowScore,
  PerplexityLanguageProfile,
} from "../types.js";

// ─── Default English Profile ─────────────────────────────────────────────────

/**
 * Built-in English language profile.
 *
 * The common trigrams are the top-50 most frequent character trigrams in
 * English text (case-insensitive). Natural English prose clusters around
 * 3.0-4.0 bits/char at the character level; adversarial suffixes typically
 * land above 5.0.
 */
const ENGLISH_PROFILE: PerplexityLanguageProfile = {
  name: "English",
  expectedRange: { min: 2.5, max: 4.0 },
  commonNgrams: [
    "the",
    "ing",
    "and",
    "ent",
    "ion",
    "tio",
    "for",
    "ati",
    "ter",
    "hat",
    "tha",
    "ere",
    "ate",
    "his",
    "con",
    "res",
    "ver",
    "all",
    "ons",
    "nce",
    "men",
    "ith",
    "ted",
    "ers",
    "pro",
    "her",
    "was",
    "one",
    "our",
    "out",
    "are",
    "not",
    "rea",
    "com",
    "eve",
    "int",
    "est",
    "sta",
    "ect",
    "ive",
    "ove",
    "ear",
    "hen",
    "ess",
    "ble",
    "per",
    "str",
    "man",
    "ine",
    "oun",
  ],
};

const DEFAULT_THRESHOLD = 4.5;
const DEFAULT_WINDOW_SIZE = 50;
const DEFAULT_NGRAM_ORDER = 3;

// ─── PerplexityAnalyzer ──────────────────────────────────────────────────────

/**
 * Character-level perplexity estimator.
 *
 * Uses n-gram frequency analysis and Shannon entropy across sliding windows
 * to detect statistically unusual character sequences. No external models,
 * no bundled weights — pure arithmetic over the input.
 *
 * @example
 * ```ts
 * const analyzer = new PerplexityAnalyzer({ threshold: 4.5 });
 * const result = analyzer.analyze("normal English sentence");
 * // result.anomalous === false, result.perplexity ~3.5
 *
 * const result2 = analyzer.analyze("Xj7#mK9@pQ2!wR5$tY8&uI0^");
 * // result2.anomalous === true, result2.perplexity >5.0
 * ```
 */
export class PerplexityAnalyzer {
  private readonly threshold: number;
  private readonly windowSize: number;
  private readonly ngramOrder: number;
  private readonly profiles: Record<string, PerplexityLanguageProfile>;
  private readonly commonNgramSet: Set<string>;

  constructor(config: PerplexityConfig = {}) {
    this.threshold = config.threshold ?? DEFAULT_THRESHOLD;
    this.windowSize = config.windowSize ?? DEFAULT_WINDOW_SIZE;
    this.ngramOrder = config.ngramOrder ?? DEFAULT_NGRAM_ORDER;

    this.profiles = {
      english: ENGLISH_PROFILE,
      ...config.languageProfiles,
    };

    // Pre-compute a set of all common n-grams across all profiles for fast lookups
    this.commonNgramSet = new Set<string>();
    for (const profile of Object.values(this.profiles)) {
      for (const ngram of profile.commonNgrams) {
        this.commonNgramSet.add(ngram.toLowerCase());
      }
    }
  }

  /**
   * Analyze an input string for perplexity anomalies.
   *
   * Splits the input into sliding windows, computes character-level n-gram
   * perplexity for each window, and flags the input as anomalous if any
   * window exceeds the configured threshold.
   *
   * @param input - The raw input string to analyze.
   * @returns A PerplexityResult with overall and per-window scores.
   */
  analyze(input: string): PerplexityResult {
    if (input.length === 0) {
      return {
        perplexity: 0,
        anomalous: false,
        windowScores: [],
        maxWindowPerplexity: 0,
      };
    }

    const lower = input.toLowerCase();

    // For inputs shorter than the window size, analyze the full string as one window
    if (lower.length < this.windowSize) {
      const score = this.computeWindowPerplexity(lower);
      const windowScore: PerplexityWindowScore = {
        start: 0,
        end: input.length,
        perplexity: score,
        text: input,
      };
      return {
        perplexity: score,
        anomalous: score > this.threshold,
        windowScores: [windowScore],
        maxWindowPerplexity: score,
      };
    }

    // Slide across the input with 75% overlap (step = windowSize / 4)
    const step = Math.max(1, Math.floor(this.windowSize / 4));
    const windowScores: PerplexityWindowScore[] = [];
    let maxPerplexity = 0;
    let totalPerplexity = 0;

    for (let i = 0; i <= lower.length - this.windowSize; i += step) {
      const windowText = lower.slice(i, i + this.windowSize);
      const score = this.computeWindowPerplexity(windowText);

      windowScores.push({
        start: i,
        end: i + this.windowSize,
        perplexity: score,
        text: input.slice(i, i + this.windowSize),
      });

      if (score > maxPerplexity) {
        maxPerplexity = score;
      }
      totalPerplexity += score;
    }

    const meanPerplexity = windowScores.length > 0 ? totalPerplexity / windowScores.length : 0;

    return {
      perplexity: meanPerplexity,
      anomalous: maxPerplexity > this.threshold,
      windowScores,
      maxWindowPerplexity: maxPerplexity,
    };
  }

  /**
   * Compute the perplexity of a single window using character n-gram
   * frequency and Shannon entropy.
   *
   * The approach combines two signals:
   * 1. Shannon entropy of the character distribution (how diverse the chars are)
   * 2. N-gram familiarity penalty (how many common language n-grams appear)
   *
   * Natural language has moderate character diversity BUT high n-gram familiarity.
   * Adversarial text has high character diversity AND low n-gram familiarity.
   */
  private computeWindowPerplexity(window: string): number {
    if (window.length === 0) return 0;

    // Step 1: Shannon entropy of character distribution
    const charEntropy = this.shannonEntropy(window);

    // Step 2: N-gram familiarity — fraction of n-grams that match common patterns
    const ngramFamiliarity = this.computeNgramFamiliarity(window);

    // Combine: high entropy + low familiarity = high perplexity.
    // Familiarity ranges from 0 (no common n-grams) to 1 (all common).
    //
    // We REDUCE entropy for familiar text: natural language contains common
    // trigrams ("the", "ing", "and") which lower the effective perplexity.
    // Adversarial text has near-zero familiarity, so it stays at raw entropy.
    //
    // The multiplier: (1.0 - familiarity), clamped to [0, 1].
    //
    // Typical values:
    //   Natural English (fam ~0.15-0.25): multiplier ~0.75-0.85 → perplexity ~3.0-3.7
    //   Adversarial text (fam ~0):        multiplier ~1.0 → perplexity = entropy
    const familiarityDiscount = Math.max(0, 1.0 - ngramFamiliarity);
    return charEntropy * familiarityDiscount;
  }

  /**
   * Calculate Shannon entropy (bits per character) for a string.
   */
  private shannonEntropy(text: string): number {
    if (text.length === 0) return 0;

    const freq = new Map<string, number>();
    for (const char of text) {
      freq.set(char, (freq.get(char) ?? 0) + 1);
    }

    let entropy = 0;
    const len = text.length;
    for (const count of freq.values()) {
      const p = count / len;
      if (p > 0) {
        entropy -= p * Math.log2(p);
      }
    }

    return entropy;
  }

  /**
   * Compute n-gram familiarity: the fraction of character n-grams in the
   * window that appear in the common n-gram set.
   *
   * Returns a value between 0 (no familiar n-grams) and 1 (all familiar).
   */
  private computeNgramFamiliarity(window: string): number {
    if (window.length < this.ngramOrder) return 0;

    let totalNgrams = 0;
    let familiarNgrams = 0;

    for (let i = 0; i <= window.length - this.ngramOrder; i++) {
      const ngram = window.slice(i, i + this.ngramOrder);
      totalNgrams++;
      if (this.commonNgramSet.has(ngram)) {
        familiarNgrams++;
      }
    }

    return totalNgrams > 0 ? familiarNgrams / totalNgrams : 0;
  }
}
